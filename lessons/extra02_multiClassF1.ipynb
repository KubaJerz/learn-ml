{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö° Quick Dip ‚ö°\n",
    "\n",
    "Here we are looking at the **inside** of the `multiclass F1 score` in PyTorch.  \n",
    "Specifically, the **'macro'** score üß†.\n",
    "\n",
    "### üéØ Macro F1:\n",
    "> Calculate metrics for each class separately and return their unweighted mean.  \n",
    "> Classes with 0 true and predicted instances are ignored üö´.\n",
    "\n",
    "I **want** the result to be given as a **log string** in Python,  \n",
    "but the string is the **raw markdown code** üìú.\n",
    "\n",
    "They claim that the function will **ignore non-present classes**,  \n",
    "however, we are about to **check** here üïµÔ∏è‚Äç‚ôÇÔ∏è.\n",
    "\n",
    "We are checking because **PyTorch** likes to throw these warnings ‚ö†Ô∏è:\n",
    "```WARNING:root:Warning: Some classes do not exist in the target. F1 scores for these classes will be cast to zeros.```\n",
    "\n",
    "\n",
    "And if we cast the F1 score to zero and still counted the class,  \n",
    "that would royally **mess up** the scores! üò± So we will **double-check** here üîç."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torcheval.metrics.functional import multiclass_f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.6667, 0.7500])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.tensor([2,2,2,2,1,1,1,0,0])\n",
    "pred = torch.tensor([2,2,2,1,1,1,2,0,0])\n",
    "\n",
    "multiclass_f1_score(pred, target, num_classes=3, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8056)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.tensor([2,2,2,2,1,1,1,0,0])\n",
    "pred = torch.tensor([2,2,2,1,1,1,2,0,0])\n",
    "\n",
    "multiclass_f1_score(pred, target, num_classes=3, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we remove all the 0 instances here, then the **macro F1** should be:\n",
    "\n",
    "$$\n",
    "\\frac{(0.75 + 0.6667)}{2} = 0.70833\n",
    "$$\n",
    "\n",
    "and not:\n",
    "\n",
    "$$\n",
    "\\frac{(0.75 + 0.6667 + 0)}{3} = 0.4722\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Warning: Some classes do not exist in the target. F1 scores for these classes will be cast to zeros.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.7083)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.tensor([2,2,2,2,1,1,1])\n",
    "pred = torch.tensor([2,2,2,1,1,1,2])\n",
    "\n",
    "multiclass_f1_score(pred, target, num_classes=3, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it works properly!! WHOOP WHOOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets double check if this woudl lead to the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4405)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target0 = torch.tensor([2,2,2,2,1,1,1,0,0])\n",
    "pred0  = torch.tensor([2,2,2,1,1,1,2,1,1])\n",
    "target1 = torch.tensor([2,2,2,2,1,1,1])\n",
    "pred1 = torch.tensor([2,2,2,1,1,1,2])\n",
    "\n",
    "target_cat = torch.concat((target0, target1))\n",
    "pred_cat = torch.concat((pred0, pred1))\n",
    "multiclass_f1_score(pred_cat, target_cat, num_classes=3, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Warning: Some classes do not exist in the target. F1 scores for these classes will be cast to zeros.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_0:0.4166666567325592\n",
      "f1_1:0.7083333730697632\n",
      "Combined: 0.5625\n"
     ]
    }
   ],
   "source": [
    "f1_0 = multiclass_f1_score(pred0, target0, num_classes=3, average='macro')\n",
    "f1_1 = multiclass_f1_score(pred1, target1, num_classes=3, average='macro')\n",
    "\n",
    "print(f'f1_0:{f1_0}')\n",
    "print(f'f1_1:{f1_1}')\n",
    "print(f'Combined: {(f1_0 + f1_1) / 2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is not perfect it is more optomistic than it really should be but is it better than micro?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6250)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target0 = torch.tensor([2,2,2,2,1,1,1,0,0])\n",
    "pred0  = torch.tensor([2,2,2,1,1,1,2,1,1])\n",
    "target1 = torch.tensor([2,2,2,2,1,1,1])\n",
    "pred1 = torch.tensor([2,2,2,1,1,1,2])\n",
    "\n",
    "target_cat = torch.concat((target0, target1))\n",
    "pred_cat = torch.concat((pred0, pred1))\n",
    "multiclass_f1_score(pred_cat, target_cat, num_classes=3, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_0:0.5555555820465088\n",
      "f1_1:0.714285671710968\n",
      "Combined: 0.634920597076416\n"
     ]
    }
   ],
   "source": [
    "f1_0 = multiclass_f1_score(pred0, target0, num_classes=3, average='micro')\n",
    "f1_1 = multiclass_f1_score(pred1, target1, num_classes=3, average='micro')\n",
    "\n",
    "print(f'f1_0:{f1_0}')\n",
    "print(f'f1_1:{f1_1}')\n",
    "print(f'Combined: {(f1_0 + f1_1) / 2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So its better than micro but still more optomistic than the real macro f1 score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
